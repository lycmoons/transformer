{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d2962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal mask 创建完成\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import utils\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformer import Transformer\n",
    "from modules import MaskedCrossEntropyLoss, LRScheduler\n",
    "from torch.optim import Adam\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "\n",
    "# 读取超参数\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "\n",
    "# 设计数据的懒加载模式\n",
    "class LazyDataset(Dataset):\n",
    "    def __init__(self, src_input_path, tgt_input_path, label_path):\n",
    "        super().__init__()\n",
    "        self.src_input = np.load(src_input_path, mmap_mode='r')\n",
    "        self.tgt_input = np.load(tgt_input_path, mmap_mode='r')\n",
    "        self.label = np.load(label_path, mmap_mode='r')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.label.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.src_input[index]), torch.tensor(self.tgt_input[index]), torch.tensor(self.label[index])\n",
    "\n",
    "\n",
    "\n",
    "# 使用 Dataset 和 DataLoader 封装数据\n",
    "src_input_path = os.path.join('processed_data', 'src_input.npy')\n",
    "tgt_input_path = os.path.join('processed_data', 'tgt_input.npy')\n",
    "label_path = os.path.join('processed_data', 'label.npy')\n",
    "dataset = LazyDataset(src_input_path, tgt_input_path, label_path)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=params['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=params['num_workers'],\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 获取可用设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# 定义模型、损失函数、优化器、学习率调度器\n",
    "vocab_size_path = os.path.join('processed_data', 'vocab_size.npy')\n",
    "vocab_size = np.load(vocab_size_path)\n",
    "model = Transformer(\n",
    "    params['num_encoder_blocks'],\n",
    "    params['num_decoder_blocks'],\n",
    "    vocab_size[0],\n",
    "    vocab_size[1],\n",
    "    params['model_size'],\n",
    "    params['dropout'],\n",
    "    params['num_heads'],\n",
    "    params['ffn_size'],\n",
    "    device\n",
    ")\n",
    "criterion = MaskedCrossEntropyLoss()\n",
    "optimizer = Adam(\n",
    "    model.parameters(),\n",
    "    params['learning_rate'],\n",
    "    (params['beta1'], params['beta2']),\n",
    "    params['eps']\n",
    ")\n",
    "scheduler = LRScheduler(optimizer, params['warmup_step'], params['model_size'])\n",
    "\n",
    "\n",
    "\n",
    "# 解码器输入的 causal mask\n",
    "tgt_causal_mask = utils.create_causal_mask(params['max_len']).unsqueeze(0).repeat(params['batch_size'], 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "# 指定使用可用设备进行模型训练\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "tgt_causal_mask = tgt_causal_mask.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "# 创建文件夹用于保存模型结果\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# 设计训练过程\n",
    "scaler = GradScaler()\n",
    "lr_histories = []\n",
    "loss_histories = []\n",
    "model.train()\n",
    "for epoch in range(1, params['num_epochs'] + 1):\n",
    "    epoch_loss = 0.0\n",
    "    for step, (x, y, l) in enumerate(dataloader):\n",
    "        print(step)\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        print(l.shape)\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        l = l.to(device, non_blocking=True)\n",
    "        sl = utils.label_smoothing(l, vocab_size[1], params['smoothing_rate'])\n",
    "        src_padding_mask = utils.create_padding_mask(x).unsqueeze(1).repeat(1, params['max_len'], 1)\n",
    "        label_padding_mask = utils.create_padding_mask(l)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output = model(x, y, src_padding_mask, tgt_causal_mask)\n",
    "            loss = criterion(output, sl, label_padding_mask)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        lr_histories.append(scheduler.step())\n",
    "        epoch_loss += loss.item()\n",
    "        print(f'成功完成第 {step} 步优化')\n",
    "    loss_histories.append(epoch_loss / params['batch_size'])\n",
    "    print(f'成功完成第 {epoch} 轮优化')\n",
    "\n",
    "    # 每 10 个 epoch 保存一下中间结果\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), os.path.join('outputs', f'model_{epoch}.pth'))\n",
    "        np.save(os.path.join('outputs', f'lr_histories_{epoch}.npy'), np.array(lr_histories))\n",
    "        np.save(os.path.join('outputs', f'loss_histories_{epoch}.npy'), np.array(loss_histories))\n",
    "        lr_histories = []\n",
    "        loss_histories = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
